<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Toasting so much toast</title>
    <link>https://blog.frenchtoastman.com/</link>
    <description>Recent content on Toasting so much toast</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Tyler French</copyright>
    <lastBuildDate>Mon, 13 Aug 2018 09:32:37 -0400</lastBuildDate>
    <atom:link href="https://blog.frenchtoastman.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>cicdallthethings</title>
      <link>https://blog.frenchtoastman.com/posts/cicdallthethings/</link>
      <pubDate>Mon, 13 Aug 2018 09:32:37 -0400</pubDate>
      
      <guid>https://blog.frenchtoastman.com/posts/cicdallthethings/</guid>
      <description>

&lt;h1 id=&#34;the-setup:8ae3c6ac9a3b5a8985ba46bd1806cab3&#34;&gt;The Setup&lt;/h1&gt;

&lt;p&gt;This article is about how to configure our Rancher test ENV.&lt;/p&gt;

&lt;h2 id=&#34;wtf-am-i-doing:8ae3c6ac9a3b5a8985ba46bd1806cab3&#34;&gt;WTF Am i doing?&lt;/h2&gt;

&lt;p&gt;This is what I want to have setup here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rancher RKE Deployment in AWS with ETCD snapshots enabled writing to &lt;code&gt;/opt/rke/snapshots&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Gitlab single node instance running on the cluster&lt;/li&gt;
&lt;li&gt;Turns out we have to scp it off the host first then do the rest on the bastin host&lt;/li&gt;
&lt;li&gt;Git init &lt;code&gt;/opt/rke/snapshots&lt;/code&gt; as a repo &lt;code&gt;etcd-snapshots&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create cron job that runs:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;git add .
if FRIDAY:
    git commit -m &amp;quot;Test Day&amp;quot;
else:
    git commit -m &amp;quot;Daily update&amp;quot;
git push origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Create CI/CD pipeline in gitlab for &lt;code&gt;etcd-snapshots&lt;/code&gt; repo that when &lt;code&gt;message=&amp;quot;Test Day&amp;quot;&lt;/code&gt; or some other logic that is better than this.

&lt;ul&gt;
&lt;li&gt;Workflow:

&lt;ul&gt;
&lt;li&gt;Deploy RKE cluster

&lt;ul&gt;
&lt;li&gt;With snapshots enabled would be same config&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;From Bastin host run following:

&lt;ul&gt;
&lt;li&gt;SSH to all and install git&lt;/li&gt;
&lt;li&gt;Run git clone of &lt;code&gt;etcd-snapshots&lt;/code&gt; to get latest backup&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;rke etcd snapshot-restore --name ~/etcd-snapshots/latest --config cluster.yml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you get Success message good

&lt;ul&gt;
&lt;li&gt;Tare down everything&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Else fail

&lt;ul&gt;
&lt;li&gt;Maybe leave it up for investigation?&lt;/li&gt;
&lt;li&gt;Maybe revert to previous state?&lt;/li&gt;
&lt;li&gt;Tare down and notifiy?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-am-i-doing-this:8ae3c6ac9a3b5a8985ba46bd1806cab3&#34;&gt;How am i doing this?&lt;/h2&gt;

&lt;p&gt;Well so what does this do for a person that is say, just responsable for kubernetes accessability? There job&amp;hellip; well ok not all of it but most of the annoying shit. Really though this eliminates the need to rely on any thing other than git and kuberenetes, two things that are already pretty married at the hip. How many times have you had to tell someone, &amp;ldquo;Oh the backups are actually curropt&amp;hellip;&amp;rdquo;, &amp;ldquo;The backups werent scheduled correctly and ran over&amp;hellip;&amp;rdquo;, or &amp;ldquo;We missed that server when we rebuilt the backup server&amp;hellip;&amp;rdquo; but hey they always open a support ticket with the vendor right? :face_palm: Git was built to manage changes in files, the snapshot of that etcd cluster you have is what again? Oh ya a file. Dont believe me, open it in vim :smile:. So if we are using this great tool as the persistent storage that for your database snapshots how are we going to trigger the actual saves? Well that is going to taken care of by a cronjob, something that has been around pretty much as long as the linux kernel. How do you get the snapshot? Its as easy as adding a few lines to your &lt;code&gt;cluster.yml&lt;/code&gt;, that will create a recurring snapshot policy and bam we have our setup!&lt;/p&gt;

&lt;h2 id=&#34;configuration:8ae3c6ac9a3b5a8985ba46bd1806cab3&#34;&gt;Configuration&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Get &lt;code&gt;cluster.yml&lt;/code&gt; for RKE deployment&lt;/li&gt;
&lt;li&gt;Deploy RKE cluster&lt;/li&gt;
&lt;li&gt;Deploy gitlab in container (optional)

&lt;ul&gt;
&lt;li&gt;Deployment in container is up and running using the config, however i cannot push to it atm ssh key issue&lt;/li&gt;
&lt;li&gt;Using gitlab.com for demo&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Configure &lt;code&gt;/opt/rke/ectd-snapshots/&lt;/code&gt; as git repo in gitlab

&lt;ul&gt;
&lt;li&gt;note that you have to use sudo so the ssh key you are using is for root&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;requirements:8ae3c6ac9a3b5a8985ba46bd1806cab3&#34;&gt;Requirements&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;git lfs&lt;/code&gt; enabled for repo

&lt;ul&gt;
&lt;li&gt;We are using &lt;code&gt;git lfs&lt;/code&gt; as in genral our DB is larger than the 50MB that GitLab starts complaining about when it comes to file size. Since we are using this we have a couple of limitations that come with it:

&lt;ul&gt;
&lt;li&gt;Max file size supported 2GB&lt;/li&gt;
&lt;li&gt;This should not be a conceren those as by default the max size the etcd cluster is set to is 2GB, it wasnt till v3.3 that the limit for a max supported limit for the DB size was increased to 10GB (here)[&lt;a href=&#34;https://coreos.com/blog/announcing-etcd-3.3&#34;&gt;https://coreos.com/blog/announcing-etcd-3.3&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;12:48 start fresh dump: 12:55 finish fresh dump

&lt;ul&gt;
&lt;li&gt;Local connection has no effect from AWS -&amp;gt; Gitlab.com&lt;/li&gt;
&lt;li&gt;Will have faster? speeds with hosted gitlab instance&lt;/li&gt;
&lt;li&gt;Speed went from 5MB to 43KB, idk would have been faster&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;each node will be working on their own branch in the repo

&lt;ul&gt;
&lt;li&gt;verification happens by diff the snapshots with the same name?&lt;/li&gt;
&lt;li&gt;or after we get a push from all 3 branches they are merged in order of FIFM(First branch in, is the first one merged)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;start-of-blog:8ae3c6ac9a3b5a8985ba46bd1806cab3&#34;&gt;Start of Blog&lt;/h2&gt;

&lt;p&gt;Insert &lt;code&gt;ci/cd all the things meme&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;What developer or engineer doesnt love &lt;code&gt;git&lt;/code&gt; for version control? I cant really think of any, that got me thinking one day. Whaten all can I use &lt;code&gt;git&lt;/code&gt; to store for me? Well it just so happen that at he time of that thought I was working on deploying &lt;code&gt;kubernetes&lt;/code&gt; into production. I had just configured &lt;code&gt;etcd&lt;/code&gt; to start taking snapshots and happen to see if I could open one up in &lt;code&gt;vim&lt;/code&gt; and when it did, it hit me. Where better to store these snapshots, the life blood of a &lt;code&gt;kubernetes&lt;/code&gt; cluster in the the same location where we are storing all the configuration, coomand line tools, and service deployments at. Then you know while we are at since we get a webhook to play with from that &lt;code&gt;git&lt;/code&gt; repo we could create for the snapshots what can we do with it? Well why not just do the very thing we want to garuentee is we can do later using our fancy pants &lt;code&gt;CI/CD&lt;/code&gt; pipelines and build out a test cluster from the snapshot. Dont worry I cant do all that in one post, so grab a beer and get to your reading spot.&lt;/p&gt;

&lt;p&gt;So first things first we are going to need a cluster to start backing up. So in this repo I have included the &lt;code&gt;cluster.yml&lt;/code&gt; file that was used to build out my test cluster. It has enough settings to get you started on defining your own cluster. Now that is sorted we some nodes to deploy to, spin up one of the supported OS&amp;rsquo;s for &lt;code&gt;rke&lt;/code&gt; deployments. I choose to go with the basic &lt;code&gt;ubuntu-16.04-lts&lt;/code&gt; image on AWS, this is because we are going to need to be able to install &lt;code&gt;git&lt;/code&gt; on these boxes for simplicity you can get real fancy with how you run the commit say a cron scheduled container? However I first attempted this I had some issues with getting &lt;code&gt;git lfs&lt;/code&gt; to run correctly with the mounted volume on the container. After all the nodes are deploy and up you are going to need to make sure in your current &lt;code&gt;PATH&lt;/code&gt; you have the &lt;code&gt;rke&lt;/code&gt; binary included in this repo. Then you only need to run the following to get the cluster up:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rke up --config /path/to/cluser.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait a few mins and everything should be gucci and you wil have an &lt;code&gt;rke&lt;/code&gt; deployment of &lt;code&gt;kubernetes&lt;/code&gt;. Next we are going to install a couple things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First we need to install &lt;code&gt;open-iscsi&lt;/code&gt; on each cluster node.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;sudo apt install open-iscsi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE&lt;/em&gt;&lt;/strong&gt; If you used the AWS image this is already installed on all nodes&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Next we are going to need to install &lt;code&gt;Longhorn&lt;/code&gt; from the catalog in the &lt;code&gt;longhorn&lt;/code&gt; namespace in our cluster. This is done by first clicking &lt;code&gt;Default&lt;/code&gt; or which ever project you wish from the cluster drop down. Then going to &lt;code&gt;Catalogs&lt;/code&gt; at the top and searching for &lt;code&gt;longhorn&lt;/code&gt; at the top right. Then you can click the deployment here and select the &lt;code&gt;namespace&lt;/code&gt; you wish to install it into and the port you wish to expose for its &lt;code&gt;UI&lt;/code&gt;. After that give it a few mins and then you will have a simple &lt;code&gt;Persistent Storage&lt;/code&gt; provider installed on the cluster. This is just one of many different &lt;code&gt;StorageClass&lt;/code&gt; drivers available for &lt;code&gt;kubernetes&lt;/code&gt; also one of the simplest to install which is why I used it for this article. Once all the containers are up for this new service we can then start making &lt;code&gt;Persistent Volume Claims&lt;/code&gt; with our &lt;code&gt;kubernetes&lt;/code&gt; services. Can you guess what the first one we are going to install is?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You may have guessed it, &lt;code&gt;GitLab&lt;/code&gt;, so using the &lt;code&gt;gitlab.yaml&lt;/code&gt; that is included in this repo you should be able to deploy a &lt;code&gt;GitLab&lt;/code&gt; instance with &lt;code&gt;80&lt;/code&gt;,&lt;code&gt;443&lt;/code&gt;, and &lt;code&gt;22&lt;/code&gt;. You may need to make some tweaks on how you have these &lt;code&gt;nodeports&lt;/code&gt; exposed to the outside world based on which loadbalancer you are working with. This &lt;code&gt;StatefulSet&lt;/code&gt; deployment will also create &lt;code&gt;3 PVC&lt;/code&gt; claims which will trigger &lt;code&gt;longhorn&lt;/code&gt; to create, by default, &lt;code&gt;3&lt;/code&gt; replicas of each claim. After those are created the main container will start up and go through its inital configuration. It is important to note that Gitlyou will need to pass the flag which enables &lt;code&gt;git lfs&lt;/code&gt; within your &lt;code&gt;GitLab&lt;/code&gt; server. After all that is finsihed you should have &lt;code&gt;GitLab&lt;/code&gt; deployed with the key directories mounted to &lt;code&gt;PVC&lt;/code&gt; claims and ready for you to create the repo we are going to be storing to.&lt;/p&gt;

&lt;p&gt;I dont really feel like writing here how to get the repo configured as its kinda outside this demo, so we will skip that here and assume your repo is configured. Now you may have seen that in the &lt;code&gt;cluster.yml&lt;/code&gt; we configured some &lt;code&gt;etcd&lt;/code&gt; snapshot guidelines. These tell the cluster to take an &lt;code&gt;etcd-snapshot&lt;/code&gt; of the cluster at a given interval and keep a certian amount of them at &lt;code&gt;/opt/rke/etcd-snapshots&lt;/code&gt;. So guess where we are going? Thats right, fire up tmux and ssh to each of the different nodes in our cluster and we are going to do the following to &lt;code&gt;git init&lt;/code&gt; the &lt;code&gt;/opt/rke/etcd-snapshots&lt;/code&gt; folder as out target for the &lt;code&gt;git repo&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We are first going create an ssh-key for the &lt;code&gt;root&lt;/code&gt; user on each node:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;sudo ssh-keygen
#Hit enter a bunch
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Then we need to run &lt;code&gt;cat /root/.ssh/id_rsa.pub&lt;/code&gt; copy the output and add all the keys as a deployment keys to the &lt;code&gt;git repo&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Now that it is added we need to run the following in &lt;code&gt;/opt/rke/etcd-snapshots&lt;/code&gt; on each node&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;git init
git remote add origin git@gitlab.frenchtoastman.com:root/etcd-snapshots
git lfs track /opt/rke/etcd-snapshots
git add .
git commit -m &amp;quot;Seed&amp;quot;
git push -u origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Wait about 5-10 mins, this varies based on the size of the cluster&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;After the above completes on each node you are going to run the following to give each node their own branch to push to:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;git checkout -b node-name
git add .
git commit -m &amp;quot;node branch seed&amp;quot;
git push -u origin node-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that compeletes on each of or nodes we now have our full seed of each &lt;code&gt;etcd&lt;/code&gt; snapshot backup. All that is left to do is build out our &lt;code&gt;CI/CD&lt;/code&gt; pipeline for what we want to do! Here is where we can get creative and do some fancy things, this is also where im going to call it on this first post. The next post about this will have to contian all the cool stuff we can do with these backups in &lt;code&gt;git&lt;/code&gt;, which could be quite a few things.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://blog.frenchtoastman.com/posts/Building/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://blog.frenchtoastman.com/posts/Building/</guid>
      <description>

&lt;h1 id=&#34;building-something-in-ci-cd:140c58546f80295ba58ab4d2981832fc&#34;&gt;Building something in CI/CD&lt;/h1&gt;

&lt;p&gt;In this post im am going to start explorering what all can be done with CI/CD pipelines in reguards to Kubernetes. So most people have been using CI/CD to deploy out their applications or modify their application that is running to fix a bug or address some feature request. But what can us infrastructure guys do with this? Well everyone seems to preach these days the infrastructure as code right? I dont really know where im going to stop with this post cause I could just cram in all everything into one or break it up but let me tell you there is a lot of stuff we can do.&lt;/p&gt;

&lt;h2 id=&#34;building-repeatable-deployments:140c58546f80295ba58ab4d2981832fc&#34;&gt;Building repeatable deployments&lt;/h2&gt;

&lt;p&gt;The first kinda key aspect of CI/CD pipelines is that through these pipelines you can script out basically any kind of deployment task that would be needed to deploy whatever application, infrastruce, or website that you want. What does that really mean though? Well lets start with an example of how to deploy out a blank kubernetes cluster.&lt;/p&gt;

&lt;p&gt;First we need to think about &amp;ldquo;Ok what do we need to do to manually build out this cluster?&amp;rdquo; so lets break it down into some steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Setup deployment environment with the tools that are needed for deployment&lt;/li&gt;
&lt;li&gt;Configure the deployment&lt;/li&gt;
&lt;li&gt;Execute the deployment&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;setup-deployment-environment:140c58546f80295ba58ab4d2981832fc&#34;&gt;Setup Deployment Environment&lt;/h3&gt;

&lt;p&gt;So here we need to gather requirments for what all it takes to deploy out a cluster. Frist you need to think about what he environement needs to look like so you can even deploy the cluster. This is where I think about the basic OS features that are required to even be able to use the tools that are available to deploy out your cluster. So first we need to be able to deploy hosts that are actually going to run our kuberenetes cluster so we are going to need tools like aws-cli, vcd-cli, or any other cloud providers cli tool. Not all cloud providers are created equal though so need to first ensure that our cloud provider acutally has a cli tool, isll get to why in a second, and with that tool we are actually able to deploy hosts of a specific image. The reason we need a cli tool to do this is that with CI/CD things we need to have a robot be able to just run some command and wait for an expected output, and if you think about it GUI&amp;rsquo;s are just for humans to interface with something. Robots just care about 1&amp;rsquo;s and 0&amp;rsquo;s so a gui is wayyyyy more than they would ever need.&lt;/p&gt;

&lt;p&gt;Ahh one thing that I havent really pointed out here is that with CI/CD pipelines you are essentinally just building a container image of the ideal deployment workstation a human would use to deploy whatever. So with that in mind we have an idea of how we are going to deploy host we need to think about what the requirments of our deployment tools are to be able to do something with these hosts. At a very low level we should think, how does the tool connect to the hosts? In most cases its going to SSH to them so we will need a tool like openssh-clients to give our container the ability to use ssh. SSH is really the only way that I know of how these tools configure these host for deployment so that is really it.&lt;/p&gt;

&lt;p&gt;Now all we need after that for a base is the deployment tool. All the tools that you would need for this are usally available on github or through some url that you can directly download it from. Oh wait so how do we download that as a robot? wget is an awesome tool that if given a direct link to the download you can download things from the command line, so lets add that to the list of tools we need to install in the container. Ok now we have the deployment tool downloaded we need some kinda configuration for the tool to apply. So how do we get that as a robot? Well we dont, not directly.&lt;/p&gt;

&lt;h3 id=&#34;setup-deployment-configuration:140c58546f80295ba58ab4d2981832fc&#34;&gt;Setup Deployment configuration&lt;/h3&gt;

&lt;p&gt;So this part is something that robots are not really good at doing, yet at least. So lets do some leg work here our selves and see just how much work we need to pre stage to be able to do this.&lt;/p&gt;

&lt;p&gt;First we need to figure out what our base configuration will be for that we need to consult some documentation for me im going to look to this doc &lt;a href=&#34;https://rancher.com/docs/rancher/v2.x/en/installation/ha/kubernetes-rke/&#34;&gt;from Rancher here&lt;/a&gt;. In this document the first thing they have is this yaml:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nodes:
  - address: 165.227.114.63
    user: ubuntu
    role: [controlplane,worker,etcd]
    ssh_key_path: ~/.ssh/id_rsa
  - address: 165.227.116.167
    user: ubuntu
    role: [controlplane,worker,etcd]
    ssh_key_path: ~/.ssh/id_rsa
  - address: 165.227.127.226
    user: ubuntu
    role: [controlplane,worker,etcd]
    ssh_key_path: ~/.ssh/id_rsa
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So this is just a list of nodes, their roles, and the user/ssh key combo we need to be able to access them. Ok great but unless we want to put more work in to create a VPC with static ip addresses or pre-stage our nodes we are not going to know the IP address that they come up with. So do we just stop here and say robots cant do this part we need to do the leg work here our selves? Maybe that would be the case if we had not been around linux for a long enough time, but if you&amp;rsquo;ve ever spent the time to really learn about the awesomeness of configuration files and built-in tools in the linux tool box you might have stumbled upon some really useful ones for the issue right here.&lt;/p&gt;

&lt;p&gt;Ill cut to the case a bit here and just go ahead and list the linus tools that you will be using to build this file on the fly:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;find - does exactly what you would think finds stuff via the linux command line &lt;code&gt;find / -name file.name&lt;/code&gt; that right there will search the &lt;code&gt;/&lt;/code&gt; filesystem for &lt;code&gt;file.name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;sed - this is where the magic comes from. &lt;a href=&#34;https://linux.die.net/man/1/sed&#34;&gt;sed&lt;/a&gt; is something that not everyone knows about but is basically a regex tool for the linux command line&lt;/li&gt;
&lt;li&gt;grep - the best linux tool you will ever use &lt;a href=&#34;https://linux.die.net/man/1/grep&#34;&gt;grep&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tr - how we will remove what we dont need, also super handy &lt;a href=&#34;https://linux.die.net/man/1/tr&#34;&gt;tr&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So the next question is how do we combine these tools to make it work for us? Well first you must look at the cli tool for your cloud provider. 99.99999999% of the time the tool that allows you to deploy these hosts will also allow you to query your VPC for information about these hosts&amp;hellip; I think you might see where im going with this. The following is an example from the &lt;a href=&#34;https://github.com/vmware/vcd-cli&#34;&gt;vcd-cli documentation&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ vcd login myserviceprovider.com org1 usr1 --password ******** -w -i
    usr1 logged in, org: &#39;org1&#39;, vdc: &#39;vdc1&#39;

    $ vcd catalog create catalog1
    task: 893bff31-4bf6-48a6-84b8-55cee97e8349, Created Catalog catalog1(cc0a2b88-9e5a-4391-936f-df6e7902504b), result: success

    $ vcd catalog upload catalog1 photon-custom-hw11-2.0-304b817.ova
    upload 113,169,920 of 113,169,920 bytes, 100%
    property    value
    ----------  ----------------------------------
    file        photon-custom-hw11-2.0-304b817.ova
    size        113207424

    $ vcd vapp create vapp1 --catalog catalog1 --template photon-custom-hw11-2.0-304b817.ova \
      --network net1 --accept-all-eulas
    task: 0f98685a-d11c-41d0-8de4-d3d4efad183a, Created Virtual Application vapp1(8fd8e774-d8b3-42ab-800c-a4992cca1fc2), result: success

    $ vcd vapp list
    isDeployed    isEnabled      memoryAllocationMB  name      numberOfCpus    numberOfVMs  ownerName    status        storageKB  vdcName
    ------------  -----------  --------------------  ------  --------------  -------------  -----------  ----------  -----------  ---------
    true          true                         2048  vapp1                1              1  usr1         POWERED_ON     16777216  vdc1

    $ vcd vapp info vapp1
    property                     value
    ---------------------------  -------------------------------------
    name                         vapp1
    owner                        [&#39;usr1&#39;]
    status                       Powered on
    vapp-net-1                   net1
    vapp-net-1-mode              bridged
    vm-1: 1 virtual CPU(s)       1
    vm-1: 2048 MB of memory      2,048
    vm-1: Hard disk 1            17,179,869,184 byte
    vm-1: Network adapter 0      DHCP: 10.150.221.213
    vm-1: computer-name          PhotonOS-001
    vm-1: password               ********
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Well would you look at that, we deployed a host from an &lt;code&gt;ova&lt;/code&gt; image of our choosing. Then we listed information about it like the IP of &lt;code&gt;Network Adapter 1&lt;/code&gt;. So I think you can see where im going with this&amp;hellip; Yep thats right we are going to deploy these hosts then take the information from the output of those commands to describe it and pipe them into the config file for our cluster. So lets look at the linux commands that will allow us to do this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vcd vapp info vapp1 | grep &amp;quot;Network adapter 0&amp;quot;
vm-1: Network adapter 0      DHCP: 10.150.221.213
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nice that gives us the line but we just need a small piece of that line so lets change that up some:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vcd vapp info vapp1 | grep &amp;quot;Network adapter 0&amp;quot; | tr -d &#39;[:space:]&#39; | awk -F&#39;:&#39; &#39;{print $3}&#39;
10.150.221.213
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And there we go we are now just grabbing the IP of our host. Things to note about this is that it &lt;em&gt;only&lt;/em&gt; works for the output of the vcd-cli commands, for aws-cli the output information will look different so we would need to tailor that command slightly to account for how that would look.&lt;/p&gt;

&lt;p&gt;So now you might be asking, &amp;ldquo;Ok great wtf does that give us?&amp;rdquo; well lets look back at the linux command line and remember this concept called &amp;ldquo;Environment Variables&amp;rdquo;. They give us a way to set values and use a &amp;ldquo;human friendly&amp;rdquo; name to address them. So lets look back at that command from above and instead of just printing the value lets make it an Environment Variable:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ NODE1=$(vcd vapp info vapp1 | grep &amp;quot;Network adapter 0&amp;quot; | tr -d &#39;[:space:]&#39; | awk -F&#39;:&#39; &#39;{print $3}&#39;)
$ echo $NODE1
10.150.221.213
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Victory is ours!!! We now have a way to set an environment variable within our container to our dynamically allocated IP address of our host. So what is next with this? Well next we are goind to need to change up our base rancher configuration like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nodes:
  - address: NODE1
    user: ubuntu
    role: [controlplane,worker,etcd]
    ssh_key_path: ~/.ssh/id_rsa
  - address: NODE2
    user: ubuntu
    role: [controlplane,worker,etcd]
    ssh_key_path: ~/.ssh/id_rsa
  - address: NODE3
    user: ubuntu
    role: [controlplane,worker,etcd]
    ssh_key_path: ~/.ssh/id_rsa
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok so now that you&amp;rsquo;ve seen that can you guess where im going with this? Well if you cant or are still reading this we are going to edit this configuration file using another classic linux tool &lt;code&gt;sed&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sed -i &amp;quot;s/NODE1/${NODE1}/g&amp;quot; rancher-config.yml |  sed -i &amp;quot;s/NODE2/${NODE2}/g&amp;quot; rancher-config.yml |  sed -i &amp;quot;s/NODE3/${NODE3}/g&amp;quot; rancher-config.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are couple things id like to note about the above command, the &lt;code&gt;-i&lt;/code&gt; flag tells sed to just edit the file instead of its default behavior to complete recreate the file. Also really important here is the &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt; around the regex used for the &lt;code&gt;sed&lt;/code&gt; command that is how you get &lt;code&gt;sed&lt;/code&gt; to put the actual value of the environment variable and not just the straight word. After we have run this command though our &lt;code&gt;rancher-config.yml&lt;/code&gt; file would look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nodes:
  - address: 10.150.221.213
    user: ubuntu
    role: [controlplane,worker,etcd]
    ssh_key_path: ~/.ssh/id_rsa
  - address: 10.150.221.214
    user: ubuntu
    role: [controlplane,worker,etcd]
    ssh_key_path: ~/.ssh/id_rsa
  - address: 10.150.221.215
    user: ubuntu
    role: [controlplane,worker,etcd]
    ssh_key_path: ~/.ssh/id_rsa
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-our-ci-cd-pipeline:140c58546f80295ba58ab4d2981832fc&#34;&gt;Create our CI/CD pipeline&lt;/h2&gt;

&lt;p&gt;So in the above sections we have covered lots of small things that gave us an understanding of what environment specific tools we will need for our ideal deployment workstation would look like, how we would like to deploy our hosts for our cluster, and how we would be able to dynamically create our &lt;code&gt;rancher-config.yml&lt;/code&gt; based on our newly deployed hosts. We now need to tie all of that together into a nice CI/CD pipeline so our robot overlords can take over and the &amp;ldquo;hard work&amp;rdquo; for us, to deploy the cluster.&lt;/p&gt;

&lt;p&gt;So frist we need to pick a CI/CD framework to use, in my case im going to what is built into gitlab. Depending on which one you pick/are required to use the syntax/style might be slightly different but the basic idea of this should still be the same. First things first we are going to need a git repo to hold our &lt;code&gt;rancher-config.yml&lt;/code&gt; and within gitlab we will also need to set some CI/CD Variables, mainly we are going to need to set our &lt;code&gt;SSH_PRIVATE_KEY&lt;/code&gt; variable so that we can actually ssh into our hosts. With AWS you pick the SSH key file you want to use when deploying the hosts however with vCD I didnt see this option when deploying the hosts so I just had to copy that file over later on not a big deal just a few extra lines in our &lt;code&gt;.gitlab-ci.yml&lt;/code&gt;. Speaking of &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; lets see what that would look like for our deployment, since it along with the &lt;code&gt;rancher-confi.yml&lt;/code&gt; are going to be the only files in our git repo.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;##
## Set container image to use
##
image: ubuntu:16.04 

##
## Create stages for gitlab runner to go through
##
stages:
  - build_cluster 
  - validate_cluster

build_deployment:
  stage: build_cluster
  script:
    ## 
    ## Install dependices 
    ##
    - apt-get -y update &amp;amp;&amp;amp; &amp;amp;&amp;amp; apt-get -y upgrade &amp;amp;&amp;amp; apt-get -y install openssh-client wget python-pip sshpass
    - pip install --user vcd-cli
    - wget https://github.com/rancher/rke/releases/download/v0.1.9/rke_linux-amd64

    ## 
    ## Set file permissions 
    ##
    - chmod +x rke_linux-amd64

    ##
    ## Create ~/.ssh directory and blank id_rsa and id_rsa.pub files
    ##
    - mkdir -p ~/.ssh
    - touch ~/.ssh/id_rsa
    - touch ~/.ssh/id_rsa.pub
    - chmod 600 ~/.ssh/id_rsa
    - chmod 600 ~/.ssh/id_rsa.pub

    ##
    ## Populate ssh key files with CI/CD variables
    ##
    - echo &amp;quot;$SSH_PRIVATE_KEY&amp;quot; | tr -d &#39;\r&#39; &amp;gt;&amp;gt; ~/.ssh/id_rsa
    - echo &amp;quot;$SSH_PUBLIC_KEY&amp;quot; &amp;gt;&amp;gt; ~/.ssh/id_rsa.pub

    ##
    ## Deploy three node cluster in vCD instance 
    ##
    - vcd login $VCD_CLOUD_PROVIDER_URL $VCD_ORG $VCD_USER --password $VCD_PASSWORD -w -i
    - vcd vapp create vapp1 --catalog $VCD_CATALOG_NAME --template $VCD_TEMPLATE_NAME --network net1 --accept-all-eulas
    - vcd vapp create vapp2 --catalog $VCD_CATALOG_NAME --template $VCD_TEMPLATE_NAME --network net1 --accept-all-eulas
    - vcd vapp create vapp3 --catalog $VCD_CATALOG_NAME --template $VCD_TEMPLATE_NAME --network net1 --accept-all-eulas

    ##
    ## Set dynamic NODE environment variables and PASSNODE variables
    ##
    - NODE1=$(vcd vapp info vapp1 | grep &amp;quot;Network adapter 0&amp;quot; | tr -d &#39;[:space:]&#39; | awk -F&#39;:&#39; &#39;{print $3}&#39;)
    - NODE2=$(vcd vapp info vapp2 | grep &amp;quot;Network adapter 0&amp;quot; | tr -d &#39;[:space:]&#39; | awk -F&#39;:&#39; &#39;{print $3}&#39;)
    - NODE3=$(vcd vapp info vapp3 | grep &amp;quot;Network adapter 0&amp;quot; | tr -d &#39;[:space:]&#39; | awk -F&#39;:&#39; &#39;{print $3}&#39;)
    - PASSNODE1=$(vcd vapp info vapp1 | grep &amp;quot;password&amp;quot; | tr -d &#39;[:space:]&#39; | awk -F&#39;d&#39; &#39;{print $2}&#39;)
    - PASSNODE2=$(vcd vapp info vapp2 | grep &amp;quot;password&amp;quot; | tr -d &#39;[:space:]&#39; | awk -F&#39;d&#39; &#39;{print $2}&#39;)
    - PASSNODE3=$(vcd vapp info vapp3 | grep &amp;quot;password&amp;quot; | tr -d &#39;[:space:]&#39; | awk -F&#39;d&#39; &#39;{print $2}&#39;)

    ##
    ## Fill in our rancher-config.yml with NODE environment variables
    ##
    - sed -i &amp;quot;s/NODE1/${NODE1}/g&amp;quot; rancher-config.yml |  sed -i &amp;quot;s/NODE2/${NODE2}/g&amp;quot; rancher-config.yml |  sed -i &amp;quot;s/NODE3/${NODE3}/g&amp;quot; rancher-config.yml

    ##
    ## Using sshpass and PASSNODE values to copy id_rsa.pub to newly deployed hosts in vCD. I know not the must secure but gotta get them there some how
    ## NOTE: If you were using AWS you wouldnt need to do this the key file would be specified on deployment
    ##
    - echo $PASSNODE1 &amp;gt;&amp;gt; passnode1
    - echo $PASSNODE2 &amp;gt;&amp;gt; passnode2
    - echo $PASSNODE3 &amp;gt;&amp;gt; passnode3
    - sshpass -f passnode1 ssh-copy-id -o StrictHostKeyChecking=no ubuntu@$NODE1
    - sshpass -f passnode2 ssh-copy-id -o StrictHostKeyChecking=no ubuntu@$NODE2
    - sshpass -f passnode3 ssh-copy-id -o StrictHostKeyChecking=no ubuntu@$NODE3

    ##
    ## Standup rancher cluster
    ##
    - ./rke_linux-amd64 up --config rancher-config.yml
  artifacts:
    paths:
    - kube_config_rancher-config.yml

validate_deployment:
  stage: validate_cluster
  script:
    ##
    ## Install dependices
    ##
    - apt-get -y update &amp;amp;&amp;amp; apt-get -y upgrade &amp;amp;&amp;amp; apt-get install -y apt-transport-https
    - curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
    - touch /etc/apt/sources.list.d/kubernetes.list 
    - echo &amp;quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&amp;quot; | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
    - apt-get update
    - apt-get install -y kubectl

    ##
    ## Set KUBECONFIG for kubectl
    ##
    - export KUBECONFIG=$(pwd)/kube_config_rancher-config.yml

    ##
    ## Validate Cluster, you can do more this is enough for me
    ##
    - kubectl get nodes
  artifacts:
    paths:
    - kube_config_rancher-config.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There we go! We now have our &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file that will do our deployment for us. Ya there are some &amp;ldquo;goofy&amp;rdquo; things that are not really production worthy where we have to pass around some ssh keys, but remember this container will be deleted after we are done so thats good right? Ok we are not quite done if you notice in the &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; there are a few variables that we are using that need to be filled in on our repo. So if you are using gitlab go to your repo and navigate to Settings -&amp;gt; CI/CD. Under there we have a Variables section and you guessed it we are going to fill in our variables there. The ones we need to fill in are the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SSH_PRIVATE_KEY
SSH_PUBLIC_KEY
VCD_CLOUD_PROVIDER_URL
VCD_ORG 
VCD_USER
VCD_PASSWORD
VCD_CATALOG_NAME
VCD_TEMPLATE_NAME
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE&lt;/em&gt;&lt;/strong&gt; I did already have the $VCD_CATALOG_NAME and $VCD_TEMPLATE_NAME already create in my vCD org so I could just pass them here, you can create/upload those in this &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; however odds are you already have those in your vCD org already.&lt;/p&gt;

&lt;p&gt;Next we just need to push all this to our git repo and sit back and watch it work. Now in some cases this is just fine others you might want to specify in your &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; some conditions on when to do certian things here. Like only validate on merges to the master branch and only deploy when there is a push to say a deploy branch or based on some tags for more info on that check out the documentation &lt;a href=&#34;https://docs.gitlab.com/ce/ci/yaml/README.html#only-and-except-simplified&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;conclusions:140c58546f80295ba58ab4d2981832fc&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;If you are a small company, lazy infrastructure dude, or just an automation guy CI/CD is for you. Learn it, live it, love it. It removes most of the human errors that happen in deployments and does not deviate from the deploy steps that are outlined. It has the power of a human but none of the fatigue when it comes to deploying/editing/destorying anything a human could from a linux command line. Embrace our robot overlords!!!!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>